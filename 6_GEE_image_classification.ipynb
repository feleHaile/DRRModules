{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "______\n",
    "\n",
    "# Land Cover Classification for use in the CAPRA Model \n",
    "\n",
    "## Image Classification\n",
    "_____"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Learning Objectives\n",
    "\n",
    "In this lesson you will learn concepts of supervised classification. You will explore processes of training data collection, classifier selection, classifier training, image classification and accuracy assessment. \n",
    "\n",
    "The purpose of this tutorial is to derive a land cover map from satellite imagery using a classification method called recursive partitioning. The procedure for the classification process is outlined in Figure 2.\n",
    "The tutorial will use the open source software Google Earth Engine (GEE) and its Python API. We will cover common tasks for data loading, image visualization and processing. The main steps of the classification process we developed for this tutorial with relevant software indicated in () are:\n",
    "\n",
    "Image classification means mapping the values captured by remote sensors that are encoded as image digital levels to specific land cover types. <br>\n",
    "Classifying remotely sensed data into a thematic map is a relevant task beacuse the resulting information is the basis for many environmental and socioeconomic applications. In this example, a crop map type is produced by applying a [*supervised*](https://en.wikipedia.org/wiki/Supervised_learning) [*Random Forest*](https://en.wikipedia.org/wiki/Random_forest) algorithm. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./imgs/JensenClfProcess.PNG\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Supervised Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A supervised classification method consists of training a classifier (algorithm) using (ground) truth data to classify  subsequently unseen data. <br>\n",
    "The process flow in a supervised classification includes basically, the following steps:\n",
    "* Collect the ground data. This data is used to train the classifier and validate its results.\n",
    "* Train the classifier.\n",
    "* Apply the classifier to produce a classified image.\n",
    "* Assess the classification accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classification Scheme Definition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The classes and the detail of a classification scheme of a land cover map are determined by its intended use.  A classification scheme can have multiple levels of detail.  It is good practice to design a classification scheme with mutually exclusive and exhaustive classes of either land cover OR land use.  For different levels or scales the class details need to be determined in a way that they can be easily aggregated such as in a hierarchical structure.  \n",
    "\n",
    "The classification scheme used in the runoff factor map delineation in the CAPRA flood model consists of a mixture of detailed land cover and land use classes as described in Table 1.  This is not an optimal scenario as overlap of classes can lead to non-exclusiveness of classes (e.g., impermeable surface and paved roads).  It is not consistent in the level of the hierarchical structure either.  The two level one forest land cover classes in the classification scheme are “natural forests” differentiated from “forests” which at the second level are further divided into  seeded or cultivated.  Further “cropped furrows”,” cereals”, “leguminous” and their subclasses can be grouped under agricultural or cropland at the first level. Roads, rangeland and rest (uncultivated) are pure land use classes, with class “rangeland” overlapping with class “pasture”.  \n",
    "\n",
    "In general it is a lot more difficult to derive land use from a remotely sensed image than it is to detect land cover, as land cover describes the surface material and therefore has a direct linkage to the spectral reflectance behavior of the material (e.g. land cover grassland vs. land use classes pasture, football field, city park).  Land use classification of detailed agricultural classes as suggested by the CAPRA classification scheme requires a very good knowledge and expertise of a region’s agricultural practices.  \n",
    "\n",
    "An important component of a land-cover classification procedure is the proper choice and delineation of training sites, used to train the computer in pattern recognition.  It is crucial to develop a database of reference points with reliable ground cover or use information at the spatial scale and thematic detailed at which the mapping procedure will be performed.  Preferably such reference points are acquired in the field (in situ) or from high resolution aerial photography.  For each of the different land cover types of interest a set of representative samples would be collected using a GPS (or digitizing technique).  \n",
    "\n",
    "In our case we do not have a database of ground reference points or aerial photography, and we do not have an intimate knowledge of agricultural practices of the region, therefore we will refer to the U.S. Geological Survey Land Use/Land Cover Classification System for use with Remote Sensor Data (Tbl. 2).  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./imgs/LCclassTable.PNG\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this tutorial we will use classes based on level one of the USGS classification system.  The classes of interest for our mapping application are: Water, Forest Land, Rangeland, Agricultural Land, Barren Land, and Urban or Built-up Land.  \n",
    "Since the classification scheme is a mix of land cover and land use we will rename classes with land use character to their corresponding land cover.  We also want to avoid spaces in the class names since we are going to use the names in code and spaces can be problematic.  The class names we are going to use instead are:\n",
    "\n",
    "(1)\twater \n",
    "(2)\tforestLand\n",
    "(3)\tcropLand\n",
    "(4)\tbuildUpLand"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question:\n",
    "#### (1) Which of the level one classes of the U.S. Geological Survey Land Use/Land Cover Classification System for use with remote                  sensor Data do you expect to be present in your ROI (google it)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remote Sensing Process\n",
    "\n",
    "One of the more common procedures to generate thematic land cover maps of a region is by applying pattern-recognition algorithms to satellite imagery in a digital image processing environment.  The use of remote sensing classification methods involve three general steps, (1) the selection and pre-processing of imagery adequate for the mapping of land cover classes of interest at their appropriate scales, (2) extraction and evaluation of spectral signatures and their statistical separability for all land cover classes of interest and development of a classification method and (3) evaluation and documentation of the mapping process and the final map product.   \n",
    "\n",
    "(1)\tDiscrimination, and mapping of various land use/land cover classes depends on the spatial resolution (pixel size); the spectral resolution (number of bands or channels and their bandwidth); the radiometric resolution (range of discrete brightness values); and the temporal resolution (return time of the sensor) in relation to the classification scheme and minimum mapping unit or appropriate scale (Teillet et al., 1997 ,Rao et al., 2007).  Since the limitations of a mapping project are often constrained by its budget, the most economic approach to consider is to use data free of charge processed with open source image processing software, which is the a solution demonstrated in this tutorial.  \n",
    "\n",
    "A large Landsat archive of 45 years of image acquisitions is available and open to the public. Landsat sensor specific information are accessible through the data portal of USGS (http://landsat.usgs.gov/Landsat_Search_and_Download.php).  For this exercise we are going to use Landsat 8 images. The Scan Line Corrector (SLC) in the Landsat 7 failed in in April of 2003 resulting in imagery with significant geometric error and considerable areas of no data value in each scene.  The Landsat 8 data source is adequate for the classes we identified in the classification scheme. Google Earth Engine (GEE) contains a variety of Landsat specific processing methods. Specifically, there are methods to compute at-sensor radiance, top-of-atmosphere reflectance, surface reflectance, cloud score and cloud-free composites.   \n",
    "    \n",
    "(2)\tThe extraction and evaluation of spectral signatures of all land cover classes of interest requires a training dataset.  In this exercise a set of training points for each class will be digitized on screen using the Google Earth application.  For the digitized sampling points spectral characteristics will then be extracted from the pre-processed imagery.  The variables we are interested in evaluating are reflectance values.  In addition we will consider derivative information, the Normalized Difference Vegetation Index (NDVI) and tasseled cap or Kauth-Thomas transformation which is based on the relationship of spectral characteristics to soil brightness, moisture content and vegetation cover.  The pre-processing and preparation of all data and the extraction of variable values for each sample of the training data set will be performed in Google Earth and GEE.\n",
    "\n",
    "The classification process is guided by the evaluation of the separability of classes for different combinations of reflectance bands and other derived variables based on the training set.  Separability analysis and feature selection are frequently based on descriptive class parameters such as the class specific mean vector and variance-covariance matrices.  For this tutorial instead of using parametric measures to evaluate class-separability, we will evaluate the effectiveness of a recursive partitioning algorithm, a non-parametric classification procedure (see pg. 38 in landCoverClassification_Theory.pdf).  Performance of different classifiers is assessed for all evaluated models based on accuracy measures derived from confusion matrices of the classified training dataset.  This initial evaluation of accuracies has a bias towards overestimating accuracies, and will only be used to select the model with the best fit.  The analysis will provide two important components for the final classification, a list of variables that are most suitable to differentiate the classes of interest, and the decision rules or classification tree (the model or classifier), that will be applied to all pixels of the entire area of interest, the Dominican Republic.  This part of the analysis will be performed in GEE Python API.      \n",
    "\n",
    "(3)\tThe final step of the mapping project is the accuracy assessment of the final classification results (the final map).  For this purpose we will use a stratified random sampling design (see pg. 45 in landCoverClassification_Theory.pdf) applied to the sampling frame of all classified image elements (pixels).  For lack of ground reference data we will use Google Earth (or Google maps) as reference source to determine classification accuracy.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ee\n",
    "from IPython.display import Image\n",
    "ee.Initialize()\n",
    "# Load the image\n",
    "# load the image collection and filter\n",
    "l8sr = ee.ImageCollection('LANDSAT/LC08/C01/T1_SR')\n",
    "\n",
    "CBD = ee.FeatureCollection(\"USDOS/LSIB/2013\")\n",
    "boundary = CBD.filterMetadata('name', 'equals', 'DOMINICAN REPUBLIC')\n",
    "\n",
    "image = ee.Image(l8sr\n",
    "            .filterDate('2018-01-01', '2019-04-30')\n",
    "            .filterBounds(boundary)\n",
    "            .sort('CLOUD_COVER')   \n",
    "            .first()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For creating simple cloud-free Landsat composites, Earth Engine provides the ee.Algorithms.Landsat.simpleComposite() method. This method selects a subset of scenes at each location, converts to TOA reflectance, applies the simple cloud score and takes the median of the least cloudy pixels. This example creates a simple composite using default parameters and compares it to a composite using custom parameters for the cloud score threshold and the percentile:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pprint\n",
    "# load the image collection and filter\n",
    "l8sr = ee.ImageCollection('LANDSAT/LC08/C01/T1_SR').filterDate('2018-01-01', '2018-12-31').filterBounds(boundary)                      \n",
    "\n",
    "composite = ee.Algorithms.Landsat.simpleComposite(l8sr)\n",
    "#pprint(composite.getInfo())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pprint\n",
    "# Load tables into feature collection\n",
    "#points = ee.FeatureCollection('ft:10q302Kafv_V_Le-jm1Cj274Ejz0kTsSWTKNxf3br').remap([1, 2, 3,4], [0, 1, 2, 3], \"class\")\n",
    "## https://fusiontables.google.com/data?docid=10q302Kafv_V_Le-jm1Cj274Ejz0kTsSWTKNxf3br\n",
    "# points = ee.FeatureCollection('ft:1p-YuR8JdqopYb-LrUxb43xzGHQyKmhGr1EJU3wku').remap([1, 2, 3,4], [0, 1, 2, 3], \"name\")\n",
    "## find table in https://fusiontables.google.com/data?docid=1Axs89HgE3yIgPoTMxTjL965OYgtBrp2i1O2waz29#rows:id=1\n",
    "points = ee.FeatureCollection('ft:1Axs89HgE3yIgPoTMxTjL965OYgtBrp2i1O2waz29')\n",
    "\n",
    "#pprint.pprint(points.getInfo())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select the bands for training\n",
    "bands = ['B2', 'B3', 'B4', 'B5', 'B6', 'B7']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample the input imagery to get a FeatureCollection of training data.\n",
    "training = image.select(bands).sampleRegions(points,['landcover'],30)\n",
    "#pprint.pprint(training.getInfo())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a Random Forest classifier and train it.\n",
    "#rf = ee.Classifier.randomForest(10).train(training,[\"landcover\"],bands)\n",
    "cart = ee.Classifier.cart().train(training,[\"landcover\"],bands)\n",
    "#print(rf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classify the input imagery.\n",
    "#result = image.select(bands).classify(rf)\n",
    "result_cart = image.select(bands).classify(cart)\n",
    "#print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "EEException",
     "evalue": "Classifier.train, argument 'classProperty': Invalid type. Expected: String. Actual: List<String>.",
     "output_type": "error",
     "traceback": [
      "Traceback \u001b[1;36m(most recent call last)\u001b[0m:\n",
      "  File \u001b[0;32m\"<ipython-input-7-1a8c76ec2b81>\"\u001b[0m, line \u001b[0;32m9\u001b[0m, in \u001b[0;35m<module>\u001b[0m\n    task.start()\n",
      "  File \u001b[0;32m\"C:\\Anaconda3\\envs\\ee_py37\\lib\\site-packages\\ee\\batch.py\"\u001b[0m, line \u001b[0;32m80\u001b[0m, in \u001b[0;35mstart\u001b[0m\n    result = data.exportImage(self._request_id, self.config)\n",
      "  File \u001b[0;32m\"C:\\Anaconda3\\envs\\ee_py37\\lib\\site-packages\\ee\\data.py\"\u001b[0m, line \u001b[0;32m818\u001b[0m, in \u001b[0;35mexportImage\u001b[0m\n    return startProcessing(request_id, params)\n",
      "  File \u001b[0;32m\"C:\\Anaconda3\\envs\\ee_py37\\lib\\site-packages\\ee\\data.py\"\u001b[0m, line \u001b[0;32m803\u001b[0m, in \u001b[0;35mstartProcessing\u001b[0m\n    return send_('/processingrequest', args)\n",
      "\u001b[1;36m  File \u001b[1;32m\"C:\\Anaconda3\\envs\\ee_py37\\lib\\site-packages\\ee\\data.py\"\u001b[1;36m, line \u001b[1;32m1186\u001b[1;36m, in \u001b[1;35msend_\u001b[1;36m\u001b[0m\n\u001b[1;33m    raise ee_exception.EEException(json_content['error']['message'])\u001b[0m\n",
      "\u001b[1;31mEEException\u001b[0m\u001b[1;31m:\u001b[0m Classifier.train, argument 'classProperty': Invalid type. Expected: String. Actual: List<String>.\n"
     ]
    }
   ],
   "source": [
    "region = boundary.geometry().bounds().getInfo()['coordinates'][0]\n",
    "task = ee.batch.Export.image.toDrive(\n",
    "  image = result_cart,\n",
    "  description = \"LCimage\",\n",
    "  #assetId = 'users/ximenamesa/LC' + \"DR\",\n",
    "  maxPixels = 100000000,   \n",
    "  #region = region,\n",
    "  scale = 2)\n",
    "task.start()\n",
    "print(\"Task started\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "EEException",
     "evalue": "Classifier.train, argument 'classProperty': Invalid type. Expected: String. Actual: List<String>.",
     "output_type": "error",
     "traceback": [
      "Traceback \u001b[1;36m(most recent call last)\u001b[0m:\n",
      "  File \u001b[0;32m\"<ipython-input-9-e3fef8d4be9b>\"\u001b[0m, line \u001b[0;32m3\u001b[0m, in \u001b[0;35m<module>\u001b[0m\n    Image(url=result_cart.getThumbUrl({'min': 1, 'max': 4}))\n",
      "  File \u001b[0;32m\"C:\\Anaconda3\\envs\\ee_py37\\lib\\site-packages\\ee\\deprecation.py\"\u001b[0m, line \u001b[0;32m32\u001b[0m, in \u001b[0;35mWrapper\u001b[0m\n    return func(*args, **kwargs)\n",
      "  File \u001b[0;32m\"C:\\Anaconda3\\envs\\ee_py37\\lib\\site-packages\\ee\\image.py\"\u001b[0m, line \u001b[0;32m248\u001b[0m, in \u001b[0;35mgetThumbURL\u001b[0m\n    return data.makeThumbUrl(data.getThumbId(params))\n",
      "  File \u001b[0;32m\"C:\\Anaconda3\\envs\\ee_py37\\lib\\site-packages\\ee\\data.py\"\u001b[0m, line \u001b[0;32m536\u001b[0m, in \u001b[0;35mgetThumbId\u001b[0m\n    return send_('/thumb', request)\n",
      "\u001b[1;36m  File \u001b[1;32m\"C:\\Anaconda3\\envs\\ee_py37\\lib\\site-packages\\ee\\data.py\"\u001b[1;36m, line \u001b[1;32m1186\u001b[1;36m, in \u001b[1;35msend_\u001b[1;36m\u001b[0m\n\u001b[1;33m    raise ee_exception.EEException(json_content['error']['message'])\u001b[0m\n",
      "\u001b[1;31mEEException\u001b[0m\u001b[1;31m:\u001b[0m Classifier.train, argument 'classProperty': Invalid type. Expected: String. Actual: List<String>.\n"
     ]
    }
   ],
   "source": [
    "#LCclip = result.clip(boundary)\n",
    "from IPython.display import Image\n",
    "Image(url=result_cart.getThumbUrl({'min': 1, 'max': 4}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clas_col = ','.join(['red','green','blue','pink'])   \n",
    "Image(url = result.getThumbUrl({'min': 1, 'max': 4, 'palette':clas_col}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Classification result could be improved by adding more data such as vegetation indices and textural features. You may try to perform a new classification considering those features."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (Spyder)",
   "language": "python3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
